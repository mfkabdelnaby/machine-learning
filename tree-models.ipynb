{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-based Models\n",
    "\n",
    "## Classification-tree\n",
    "\n",
    "* Sequence of if-else questions about indivisual features to infer class labels\n",
    "* Able to capture non-linear relationships between features and labels\n",
    "* Don't require feature scaling\n",
    "\n",
    "\n",
    "<font color ='brown'>**Some Definisions:**</font>\n",
    "* **Decision-Tree:** data structure consisting of a hierarchy of nodes\n",
    "* **Node:** a point that involves a question or prediction\n",
    "    * **Root:** a node where tree starts growing; a question giving rise to *two* children nodes through **branches**, no parent node\n",
    "    * **Internal node:** *one* parent node, question giving rise to *two* children nodes\n",
    "    * **Leaf:** *one* parent node, no children nodes >> prediction\n",
    "    \n",
    "When an internal node is split, the split is performed in such a way so that **information gain** is maximized. That's how it choses the feature and point to split on. When the information gain resulting from splitting a node is null, the node is declared as a leaf.\n",
    "\n",
    "<font color='brown'>**Classification Tree Parameters:**</font>\n",
    "\n",
    "**`DecisionTreeClassifier`**\n",
    "* **`max_depth`** represents the depth of each tree\n",
    "* **`random_state`** a seed to inform reproducability\n",
    "* **`min_samples_leaf`** restriction of the min percentage of training data in the leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#set plotting style to 'ggplot'\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ignore some warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** You will work with _Wisconsin Breast Cancel Dataset_ in which you'll predict whether a tumor is malignant or benign based on two features: the mean radius of the tumor (`radius_mean`) and its mean number of concave points (`concave points_mean`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('datasets\\wisconsin-breast-cancer.csv')\n",
    "X = df[['radius_mean','concave points_mean']].values\n",
    "y = df['diagnosis'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B' 'M' 'M' 'B' 'B']\n"
     ]
    }
   ],
   "source": [
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into 30% test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,\n",
    "                                                random_state=1,stratify=y)\n",
    "\n",
    "# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
    "dt = DecisionTreeClassifier(max_depth=6, random_state=1)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "print(y_pred[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute test set accuracy  \n",
    "acc = accuracy_score(y_pred, y_test)\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='brown'>**Criterions for node split**</font>\n",
    "\n",
    "1. Entropy\n",
    "2. Gini index (default)\n",
    "\n",
    "They produce same accuracy but *gini* tends to be slightly faster to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using entropy:  0.8947368421052632\n",
      "Accuracy achieved by using the gini index:  0.9005847953216374\n"
     ]
    }
   ],
   "source": [
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import accuracy_score from sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Instantiate dt_entropy, set 'entropy' as the information criterion\n",
    "dt_entropy = DecisionTreeClassifier(max_depth=8, criterion ='entropy', random_state=1)\n",
    "\n",
    "# Fit dt_entropy to the training set\n",
    "dt_entropy.fit(X_train, y_train)\n",
    "\n",
    "# Use dt_entropy to predict test set labels\n",
    "y_pred= dt_entropy.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy_entropy\n",
    "accuracy_entropy = accuracy_score(y_pred, y_test)\n",
    "\n",
    "# Print accuracy_entropy\n",
    "print('Accuracy achieved by using entropy: ', accuracy_entropy)\n",
    "\n",
    "# Print accuracy_gini\n",
    "print('Accuracy achieved by using the gini index: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree for regressiom\n",
    "\n",
    "Recall that in regression the target variable is continuous.\n",
    "\n",
    "In the next exercise you will use decision trees to predict the `mpg` value using 6 features as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mpg  displ   hp  weight  accel  origin  size\n",
      "0  18.0  250.0   88    3139   14.5      US  15.0\n",
      "1   9.0  304.0  193    4732   18.5      US  20.0\n",
      "2  36.1   91.0   60    1800   16.4    Asia  10.0\n",
      "3  18.5  250.0   98    3525   19.0      US  15.0\n",
      "4  34.3   97.0   78    2188   15.8  Europe  10.0\n"
     ]
    }
   ],
   "source": [
    "# Read file\n",
    "df = pd.read_csv('datasets\\mpg.csv')\n",
    "print(df.head())\n",
    "\n",
    "df_region = pd.get_dummies(df)\n",
    "\n",
    "X = df_region.drop('mpg',axis=1).values\n",
    "y = df_region['mpg'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot scatter\n",
    "\n",
    "plt.scatter(df['mpg'],df['displ'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the relationship between these two variables is *non-linear*; therefore, a linear regression model is going to fail modeling and predicting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of dt: 4.37\n"
     ]
    }
   ],
   "source": [
    "# Import DecisionTreeRegressor from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Split train and test data\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,\n",
    "                                                random_state=3)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeRegressor(max_depth=8,\n",
    "             min_samples_leaf=0.13,\n",
    "            random_state=3)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Compute y_pred\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute mse_dt\n",
    "mse_dt = MSE(y_pred, y_test)\n",
    "\n",
    "# Compute rmse_dt\n",
    "rmse_dt = np.sqrt(mse_dt)\n",
    "\n",
    "# Print rmse_dt\n",
    "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## Generization Error\n",
    "\n",
    "The very basic idea of machine learning is the find a function $f$ that ralates the features to the label. So we find a model that best approximates $f: \\hat{f} \\approx f$.\n",
    "\n",
    "$\\hat{f}$ can be any classification/regression model out there. Therefore, the main goal is that the model should achieve low predictive errors on unseen datasets.\n",
    "\n",
    "You may encounter two difficulties when approximating $\\hat{f(x)}$:\n",
    "* $\\hat{f(x)}$ fits training set noise --> **overfitting**\n",
    "* $\\hat{f(x)}$ is not flexible enough to approximate $f$ --> **underfitting**\n",
    "\n",
    "**Overfitting**\n",
    "* Low training set error and high test set error\n",
    "\n",
    "**Underfitting**\n",
    "* Training set error is roughly equal to test set error, but both errors are high\n",
    "\n",
    "-----------\n",
    "**Generization Error** \n",
    "it is how much the model generalizes unseen data\n",
    "\n",
    "$\\hat{f(x)} = bias^2 + variance + irreducible \\space error$\n",
    "\n",
    "* **Bias** error term that tells you, on average, how much $\\hat{f} \\neq f$. High values lead to underfitting\n",
    "* **Variance** how much $\\hat{f}$ in inconsistent over different training sets. High values lead to overfitting\n",
    "* **Irreducible Error** error of the noise\n",
    "\n",
    "As the model complexity increases, bias decreases, and variance increases. Thus, you need to chose a complexity that balances bias and variance; therefore, minimizes the generalization error.\n",
    "\n",
    "So how do we diagnose bais and variance problems??\n",
    "> use _Cross-Validation (CV)_\n",
    "\n",
    "Perform the k-fold grid search and get the average error.\n",
    "* if CV error > training set error --> **hight variance**\n",
    "\n",
    "To remedy this overfitting, decrease model complexity, or gatter more data\n",
    "\n",
    "* if CV error $\\approx$ training set error >> desired error\n",
    "To remedy this underfitting, increase model complexity, or gatter more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set SEED for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split the data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Instantiate a DecisionTreeRegressor dt\n",
    "dt = DecisionTreeRegressor(min_samples_leaf=0.26, max_depth=4, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the 10-fold CV error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV RMSE: 5.14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Compute the array containing the 10-folds CV MSEs\n",
    "MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, \n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       n_jobs=-1)\n",
    "\n",
    "# Compute the 10-folds CV RMSE\n",
    "RMSE_CV = (MSE_CV_scores.mean())**((1/2)) #sqrt\n",
    "\n",
    "# Print RMSE_CV\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very good practice is to keep the test set untouched until you are confident about your model's performance. CV is a great technique to get an estimate of a model's performance without affecting the test set.\n",
    "\n",
    "Now evaluate the **error of the training set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 5.15\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the training set\n",
    "y_pred_train = dt.predict(X_train)\n",
    "\n",
    "# Evaluate the training set RMSE of dt\n",
    "RMSE_train = (MSE(y_train,y_pred_train))**((1/2))\n",
    "\n",
    "# Print RMSE_train\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the training set error and CV error are roughly equal, but they are higher than the test set error achieved in the previous exercise. Thus, the model suffers from high bias (underfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Ensemble Learning\n",
    "\n",
    "Decision Trees (CARTS) have many advantages such as:\n",
    "* Easy to use\n",
    "* Easy to interpret their results\n",
    "* Don't require scaling or a lot of pre-processing\n",
    "* Model non-linear relationships\n",
    "\n",
    "On the other hand they have some limitations:\n",
    "* Sensitive to small variations on training set\n",
    "* Unconstrianed CARTs may overfit the training set\n",
    "\n",
    "One solution to reduce these limitations is **Ensemle Learning**. It is main concept is to train different models on the same dataset, aggregate their predictions, and predict the class that gets the most votes (**Voting Classifiers**). This results in a final model that is more robust and less prone to errors.\n",
    "\n",
    "In the next exercise you will predict weather a patient suffers from liver disease or not based on some features. Notice that the dataset is already pre-processed by standarizing its numerical feature. The original dataset is availabe in the same folder in case you want to work with it instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age_std</th>\n",
       "      <th>Total_Bilirubin_std</th>\n",
       "      <th>Direct_Bilirubin_std</th>\n",
       "      <th>Alkaline_Phosphotase_std</th>\n",
       "      <th>Alamine_Aminotransferase_std</th>\n",
       "      <th>Aspartate_Aminotransferase_std</th>\n",
       "      <th>Total_Protiens_std</th>\n",
       "      <th>Albumin_std</th>\n",
       "      <th>Albumin_and_Globulin_Ratio_std</th>\n",
       "      <th>Is_male_std</th>\n",
       "      <th>Liver_disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.247403</td>\n",
       "      <td>-0.420320</td>\n",
       "      <td>-0.495414</td>\n",
       "      <td>-0.428870</td>\n",
       "      <td>-0.355832</td>\n",
       "      <td>-0.319111</td>\n",
       "      <td>0.293722</td>\n",
       "      <td>0.203446</td>\n",
       "      <td>-0.147390</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.062306</td>\n",
       "      <td>1.218936</td>\n",
       "      <td>1.423518</td>\n",
       "      <td>1.675083</td>\n",
       "      <td>-0.093573</td>\n",
       "      <td>-0.035962</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>0.077462</td>\n",
       "      <td>-0.648461</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.062306</td>\n",
       "      <td>0.640375</td>\n",
       "      <td>0.926017</td>\n",
       "      <td>0.816243</td>\n",
       "      <td>-0.115428</td>\n",
       "      <td>-0.146459</td>\n",
       "      <td>0.478274</td>\n",
       "      <td>0.203446</td>\n",
       "      <td>-0.178707</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.815511</td>\n",
       "      <td>-0.372106</td>\n",
       "      <td>-0.388807</td>\n",
       "      <td>-0.449416</td>\n",
       "      <td>-0.366760</td>\n",
       "      <td>-0.312205</td>\n",
       "      <td>0.293722</td>\n",
       "      <td>0.329431</td>\n",
       "      <td>0.165780</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.679294</td>\n",
       "      <td>0.093956</td>\n",
       "      <td>0.179766</td>\n",
       "      <td>-0.395996</td>\n",
       "      <td>-0.295731</td>\n",
       "      <td>-0.177537</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>-0.930414</td>\n",
       "      <td>-1.713237</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age_std  Total_Bilirubin_std  Direct_Bilirubin_std  \\\n",
       "0  1.247403            -0.420320             -0.495414   \n",
       "1  1.062306             1.218936              1.423518   \n",
       "2  1.062306             0.640375              0.926017   \n",
       "3  0.815511            -0.372106             -0.388807   \n",
       "4  1.679294             0.093956              0.179766   \n",
       "\n",
       "   Alkaline_Phosphotase_std  Alamine_Aminotransferase_std  \\\n",
       "0                 -0.428870                     -0.355832   \n",
       "1                  1.675083                     -0.093573   \n",
       "2                  0.816243                     -0.115428   \n",
       "3                 -0.449416                     -0.366760   \n",
       "4                 -0.395996                     -0.295731   \n",
       "\n",
       "   Aspartate_Aminotransferase_std  Total_Protiens_std  Albumin_std  \\\n",
       "0                       -0.319111            0.293722     0.203446   \n",
       "1                       -0.035962            0.939655     0.077462   \n",
       "2                       -0.146459            0.478274     0.203446   \n",
       "3                       -0.312205            0.293722     0.329431   \n",
       "4                       -0.177537            0.755102    -0.930414   \n",
       "\n",
       "   Albumin_and_Globulin_Ratio_std  Is_male_std  Liver_disease  \n",
       "0                       -0.147390            0              1  \n",
       "1                       -0.648461            1              1  \n",
       "2                       -0.178707            1              1  \n",
       "3                        0.165780            1              1  \n",
       "4                       -1.713237            1              1  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets\\indian_liver_patient\\indian_liver_patient_preprocessed.csv'\n",
    "                 ,index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Liver_disease',axis=1).values\n",
    "y = df['Liver_disease'].values\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,\n",
    "                                                random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED=1\n",
    "\n",
    "# Instantiate lr\n",
    "lr = LogisticRegression(random_state=SEED)\n",
    "\n",
    "# Instantiate knn\n",
    "knn = KNN(n_neighbors=27)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)\n",
    "\n",
    "# Define the list classifiers\n",
    "classifiers = [('Logistic Regression', lr), \n",
    "               ('K Nearest Neighbours', knn), \n",
    "               ('Classification Tree', dt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : 0.764\n",
      "K Nearest Neighbours : 0.701\n",
      "Classification Tree : 0.730\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the pre-defined list of classifiers\n",
    "for clf_name, clf in classifiers:    \n",
    " \n",
    "    # Fit clf to the training set\n",
    "    clf.fit(X_train, y_train)    \n",
    "   \n",
    "    # Predict y_pred\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_pred, y_test) \n",
    "   \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: 0.770\n"
     ]
    }
   ],
   "source": [
    "# Import VotingClassifier from sklearn.ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Instantiate a VotingClassifier vc\n",
    "vc = VotingClassifier(estimators=classifiers)     \n",
    "\n",
    "# Fit vc to the training set\n",
    "vc.fit(X_train, y_train)   \n",
    "\n",
    "# Evaluate the test set predictions\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "print('Voting Classifier: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the voting classifier achieves a test set accuracy of 77.0%. This value is greater than that achieved by LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "It stands for **Bootstrap Aggregation**. In bagging only one algorithm is used on different subsets of the training set. Those subsets are obtained by bootstrabbing subsets of the training set with replacement. This reduces variance that leads to over-fitting. The final precition is decided by majority voting in case of classification (`BaggingClassifier`), and by averaging in regression (`Baggingregressor`).\n",
    "\n",
    "```python\n",
    "BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1)\n",
    "```\n",
    "This codes uses bagging with dicision tree classifier consisting of 50 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1,\n",
    "                      bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy of bc: 0.69\n"
     ]
    }
   ],
   "source": [
    "# Fit bc to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate acc_test\n",
    "acc_test = accuracy_score(y_pred, y_test)\n",
    "print('Test set accuracy of bc: {:.2f}'.format(acc_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Bag Evaluation\n",
    "\n",
    "Using Bagging classifiers samples, on average, 63% of the training instances. The remaining 37% constitute the **Out-of-Bag (OOB)** instances. The OOB samples are applied to their corresponing bagging, and the OOB score (accuracy for classifiers, R^2 for regressors) is calculated for each bagging using the OOB samples. The final OOB score is the average of all OOB indivual scores.\n",
    "\n",
    "**`oob_score = True`** inside **`BaggingClassifier`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=8, random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, \n",
    "            n_estimators=50,\n",
    "            oob_score=True,\n",
    "            random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.718, OOB accuracy: 0.686\n"
     ]
    }
   ],
   "source": [
    "# Fit bc to the training set \n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "acc_test = accuracy_score(y_pred, y_test)\n",
    "\n",
    "# Evaluate OOB accuracy\n",
    "acc_oob = bc.oob_score_\n",
    "\n",
    "# Print acc_test and acc_oob\n",
    "print('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set accuracy and OBB scores are close!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## Random Forests\n",
    "\n",
    "Another ensemle method that uses Decision Tress as base estimator. Each estimator is trained on a different bootstrap sample having the same size as the training set. \n",
    "\n",
    "It introduces further randomization in the training of indivisual trees because only $d$ features are sampled at each node without replacement ($d$ < total number of features). By detault, it is the square root of the total number of features.\n",
    "\n",
    "* **`RandomForestClassifier`** aggregates prediction using majority voting\n",
    "* **`RandomForestRegressor`** aggregates prediction using averaging\n",
    "\n",
    "\n",
    "In the next example, you will predict bike rental demand through different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "      <th>instant</th>\n",
       "      <th>mnth</th>\n",
       "      <th>yr</th>\n",
       "      <th>Clear to partly cloudy</th>\n",
       "      <th>Light Precipitation</th>\n",
       "      <th>Misty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>452</td>\n",
       "      <td>14487</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>356</td>\n",
       "      <td>14488</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.1940</td>\n",
       "      <td>303</td>\n",
       "      <td>14489</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.1940</td>\n",
       "      <td>277</td>\n",
       "      <td>14490</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>174</td>\n",
       "      <td>14491</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      hr  holiday  workingday  temp   hum  windspeed  cnt  instant  mnth  yr  \\\n",
       "1483  19        0           1  0.80  0.49     0.1343  452    14487     8   1   \n",
       "1484  20        0           1  0.80  0.49     0.1343  356    14488     8   1   \n",
       "1485  21        0           1  0.76  0.58     0.1940  303    14489     8   1   \n",
       "1486  22        0           1  0.76  0.58     0.1940  277    14490     8   1   \n",
       "1487  23        0           1  0.74  0.62     0.1045  174    14491     8   1   \n",
       "\n",
       "      Clear to partly cloudy  Light Precipitation  Misty  \n",
       "1483                       1                    0      0  \n",
       "1484                       1                    0      0  \n",
       "1485                       1                    0      0  \n",
       "1486                       1                    0      0  \n",
       "1487                       1                    0      0  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/bike.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('cnt',axis=1)\n",
    "y = df['cnt']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,\n",
    "                                                random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of rf: 54.49\n"
     ]
    }
   ],
   "source": [
    "# Import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate rf\n",
    "rf = RandomForestRegressor(n_estimators=25,\n",
    "            random_state=2) #you can also specify min_sample_leaf\n",
    "            \n",
    "# Fit rf to the training set    \n",
    "rf.fit(X_train, y_train) \n",
    "\n",
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test,y_pred)**((1/2))\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "The features that are most predictive according to the `rf` model trained earlier. In a single decision tree, important features are likely to appear closer to the root of the tree, while unimportant features will often appear closer to the leaves (or not at all).\n",
    "\n",
    "**`rf.feature_importances_`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAAEJCAYAAADPW83kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XmYXGWZ/vHva8ImQeIYQRLACMYFooAsAoIsruACKtyAGxEk44o4MIgz/jRuI24sCioBBZX1kUUyrDoIyL4nbO6CAo0IiEiEBBLq98f7thRNdVclp1LVJ31/rquvrrPfXZ300+85p8+TGo0GZmZmtvSe1e8AZmZmdediamZmVpGLqZmZWUUupmZmZhW5mJqZmVXkYmpmZlaRi6mZmVlFLqZmXZRSOiGl1GjxsWeXj7MopTSjm/vstpTS1PK1b9PvLCNJKR2XUrqk3zms3sb3O4DZcugyQEPm/b0fQTqRUlqx0Wg83u8cvZZSehaQ+p3Dlg8emZp13+ONRuMvQz4WDC5MKe2ZUpqbUlqQUrozpXRYSmnVpuVvSCldklL6W0rp4ZTSpSmlLZqW3wmMA44fHPmW+TNSSouag6SU1i7rbF+mty/Tb0kpXZ5SWgDMLMs2TSn9LKU0P6V0f0rpzJTSC4fs64yU0gMppcdSSn9MKf1np29K00j13SmlC1NKj6aUfp1S2i6lNCWldF5K6Z8ppdtTSts2bTeY+W0ppWvL+3ZbSukNQ/a/ZUrplyXbQymlk1NKazQtn5VS+n1KaY+U0q+Bx4HTgH2B7ZrOIswo63+ifJ/mp5T+klI6NaW0VotcbyjHfbRkf9OQXGuklI5PKd1Xsv8mpbRP0/IXl/f17yX3z1JKr2ha/pyy/V9SSgtTSnellA7r9H233nAxNeuh8oP6u8A3gQ2A9wOvB77XtNoE4GhgS2Br4HfABSml55XlmwOLgQOAtcrHkvom8DXg5cBPU0obAJcCVwGbATuWY/w8pbRy2eY7wOol78vJRejupTj2F8nvwcbAr4BTgB8CxwKblHknp5RWGLLdYcAXyjpXA3NSSlMAUkovAH5W8mwBvA2YDpwxZB+TgY8AM8jv/weAk8vXPfhenta0/kHAK4B3AOsCp7b4er4B/A+wEXA9cFpKaWLJtQr5fd0IeE855seBR8vyNYHLgb8C25K/578BLkkpPb/s/0vAq4BdgGnAHuU9stGk0Wj4wx/+6NIHcAKwCJjf9PGHpuV3Ah8ass1rgQbw3GH2+SzgIeA9TfMWATOGrDcDWDRk3tpl39uX6e3L9Pta5D51yLyVyD/0dy3T84BZS/BeTC3H2mbI9AFN62xe5h3YNG+TMm/6kMz7Nq0zHvgT8KUy/UVyIV2xaZ2NynavLdOzgCeBdYfkPA64pIOvZzDXlCG53tm0zgvKvDeV6X2BBcDaw+xzFnD1kHkJ+MPg+wScDZzQ73/b/hj5w9dMzbrvGmDvpulFAGWk8ULgsJTSN5qWD163ezFwXUrpReQR2FbAGuRi+uyybbdcO2R6c+DFKaX5Q+avTB4NARwBHJNS2gm4BDi30Wj8cimOPa/p9V/K55tbzFuDp7tq8EWj0ViUUrqWPNID2JBclB5vWmdeSunhsmww532NRuPPnYQsp8Y/XY4xkafO5L0QuKdp1blNx/xLSmkxsGaZtSlwe6PRGG4EvzmwaYv3fRWeet+/A5yRUtoMuAi4ALiw0Wg82cnXYb3hYmrWfY81Go3ft5g/+MP4E8DFLZYP/sA9B3gA+ChwF/na3uXAim2O2+qH69BTpYP+2SLbj4FDW6z7IECj0Tg+pXQB8GZgB+D8lNJZjUbjvW1yDfVE0+vGCPPaXYYaevPQcC2wmucP/bpb7zildYHzyO/JF8jfj7WB/+OZ34dWN281Zx+pNdezyAXyYy2WPQzQaDQuLHneRB4NnwjcklJ6XaPRWNz2i7GecDE165FGo3FfSuku4KWNRuPYVuuU66IbADs3Go0Ly7y1eeYo7XHyTUjN/gqMSymt2Wg07ivzXtVhvOuBV5JPSQ/7w7/RaNwLHE+++ek84JSU0kcajcY/OjxOFVsCtwOklMaTR3UnlmW3AR9ITXcmp5Q2Il/jva3Nflu9l5uTR4cHNBqNx8r+Nl2KzDcA+6SU1h5mdHo9+fT8PYPHaaXRaPyNfG35lJTS8eRR+gbALUuRyZYB34Bk1lv/DeyfUvpMSml6SumlKaVdU0rHlOUPAfcD+6WUXpJS2or8Q3ToD9o7gB1SSpNTSpPKvGuBR4BDU0rTUkpvBj7bYa7/Id9UdGJKaYuU0otSSjuklI5MKa0HkFI6KqW0c0pp/ZTShsA7ySPnR5burVhih5Tjv5x8A9Oa5TPAUcBzgBPK+7oNeVR5eaPRuKzNfu8AXpZS2jClNCmltBL5pq8GcGB5L3al8/ey2Snka7tzUkqvL/t6XUppj6bc48g3gW2b8h3P26SUvpxS2hqgvH5n+bcyjXwj03ygo9PV1hsupmY91Gg0fkz+G9S3kIvfdeSbUO4py58EdgfWJ19HPIF8rfLeIbs6kHw97g5y8R0cvexFHsHdDPw/4OAOc/2KfOfwBOBC8gjwWPLobPBvZFPJciv5GuSqwE4jjWS77CDyjUZzgdcAuwyO9spI/I3kU7HXkU+V3wq8q4P9fr9scyX5vdyr0WjcTL7r9t/J78VB5Lunl0ij0XgU2K5kOZV8F+7R5Pd1MPdW5NPIZ5Lv5D2JfF128Hu+gHyq+QaeOoOwU6PReHhJ89iyk3r3/8DMbMmVG4EuBtYZ4UYes77yyNTMzKwiF1MzM7OKfJrXzMysIo9MzczMKvLfmY4dPgVhZrZ02nYXcjEdQwYGBvodYUSTJk3igQce6HeMEdUhI9QjpzN2Rx0yQj1ytso4efLkjrb1aV4zM7OKXEzNzMwqcjE1MzOryH8aM3Y0SG2voZuZLVcG7rmn/UrFCNdM2/7w9Mi0BiRNlXRrv3OYmVlrLqbLCUm+M9vMrE/8A7g+xkk6ltzZ4x5gF+B8cqeL1wBzgG/2L56Z2djlYlof04C9ImI/ScFTraUmRsR2rTaQNBOYCRARvUlpZjaKTJo0qf1Kxfjx45do/adtu1RbWT/cERFzy+sbgKnl9WnDbRARs4HZZdJ3mpnZmLMkD4rwQxvGhoVNrxfz1C9C/+xDFjMza+JiamZmVpGLqZmZWUV+aMPY0fCD7qurQ0aoR05n7I46ZIR65PRDG8zMzPrIxdTMzKwiF1MzM7OKXEzNzMwqcjE1MzOryMXUzMysIhdTMzOzilxMzczMKvKD7seQyVOm9DtCW509UnpkA/fc04W9mJl1ziPTJSDpTknP6M8j6cplfQwzMxu9XEw7JGnccMsiYuteZjEzs9FlTJzmlXQwsCAiviXpcGCjiNhR0uuADwDnAv9Ffv7iuRHxqbLdfOAw4E3AgU37WwU4CzgjIo6VND8iJkjaHpgFPABMJ/cdfW9ENCTtXPb1AHAjsF5EvFXS84BTgOcD19L0DEhJPwXWAVYGjoyI2ZL2BaZHxCfLOvsBL4+I/+j+O2dmZp0YKyPTXwLbltebARMkrQBsA/wO+CqwI7AxsLmkXcu6qwK3RsSrI+LyMm8C8L/AyRFxbItjbQIcAGwArAe8RtLKwDHAThGxDblwDvoccHlEbALMAdZtWrZPRGxaMu9fCu+pwNtLfsi/DBy/xO+ImZl1zZgYmZJHiJtKWo3cZPtGcoHallwYL4mI+wEknQS8FvgpuQn3GUP2dTbwtYg4aZhjXRsRd5d9zQWmAvOBP0bEHWWdU4CZ5fVrgXcCRMS5kh5q2tf+kt5RXq8DTIuIqyX9AnirpF8BK0TELa2CSJo5eJyIGO69We5MmrTsLjmPHz9+me6/W+qQ0xm7ow4ZoR45q2QcE8U0Ip6QdCd5FHclcDOwA7A+8Gdg02E2XRARi4fMuwLYSdLJEdGqf93CpteLye9xu/Y9z9hPOWX8emCriHhU0iXk070Ax5FPS/+aEUalETEbmD3cMZZXy7LNUx3aSEE9cjpjd9QhI9Qj5wgt2NoaK6d5IZ/qPah8vgz4EDAXuBrYTtKkcpPRXsClI+zns8CDwHeW4Ni/BtaTNLVM7zEk13sAJO0EPLfMXx14qBTSlwFbDm4QEdeQR6rvJo9yzcysj8ZSMb0MWAu4KiLuAxYAl0XEvcCngYuBecCNEXF2m30dAKws6WudHDgiHgM+Alwg6XLgPuDhsvjzwGsl3Qi8kTxSBrgAGC/pZuCL5KL/tN0CV0TEQ5iZWV+lRmPMnP3rK0kTImK+pAQcDfwuIg6vsL9zgMMj4qION2kMDAws7eF6oq6ngUajOuR0xu6oQ0aoR84RTvO2u1Q3Nq6ZjhL7SdobWBG4iXx37xKTNJH8JzTzlqCQmpnZMuRi2iNlFLrUI9Gm/fwdeEn1RGZm1i1j6ZqpmZnZMuFiamZmVpGLqZmZWUUupmZmZhW5mJqZmVXkYmpmZlaR/zRmDJk8ZcqIywfuuadHSczMli8emZqZmVXkYtoHkqZKurXfOczMrDtcTM3MzCryNdP+GSfpWGBr4B5gF+B84KCIuF7SJOD6iJgqaQawKzAOmA58k/yM3/eR+6fuHBF/68PXYGZmuJj20zRgr4jYT1IA72qz/nRgE3KD8N8Dn4qITSQdDrwfOGLoBpJmAjMBIqJtoKXtMN8tVbrc90odMkI9cjpjd9QhI9QjZ5WMLqb9c0dEzC2vbwCmtln/4oh4BHhE0sPA/5b5twCvbLVBRMwGZpfJtr32+t0eqa4tmkajOuR0xu6oQ0aoR84RWrC15Wum/bOw6fVi8i82i3jqe7LyCOs/2TT9JP6lyMysr1xMR5c7gU3L6936mMPMzJaARzSjyzeAkPQ+4Bfd3rkfymBmtmykRqPtpTRbPjQGBgb6nWFEdb2mMhrVIaczdkcdMkI9co5wzTS129anec3MzCpyMTUzM6vIxdTMzKwiF1MzM7OKXEzNzMwqcjE1MzOryMXUzMysIj+0YQyZPGXKiMv9UAczs6XjkamZmVlFLqY9IGmipI/0O4eZmS0bLqa9MRFwMTUzW075mmlvHAqsL2ku8HPgr4CAlYCzIuJzkqYCFwCXA1sC84Djgc8DawDviYhrJc0C1gemAOsAX4uIY3v75ZiZWTM/6L4HSqE8JyKmS3ojub3av5MfnjwH+BrwZ+D3wCbAbcB15IK6L/B24AMRsWsppu8gF9xVgZuAV0fEM55iL2kmMBMgIjYljfys5scXLhxx+bI2fvx4Fi1a1NcM7dQhI9QjpzN2Rx0yQj1ytsq44oorQgcPuvfItPfeWD5uKtMTgGnkYnpHRNwCIOk24KKIaEi6BZjatI+zI+Ix4DFJFwNbAD8deqCImA3MLpNtf2vqd0eHunaVGI3qkNMZu6MOGaEeOUfoGtOWi2nvJeArEXFM88wyem0eGj7ZNP0kT/9eDS2MPr1gZtZHvgGpNx4BViuvLwT2kTQBQNIUSWss4f52kbSypOcB25NPCZuZWZ94ZNoDEfGgpCsk3QqcD5wMXCUJYD7wXmDxEuzyWuBcYF3gi62ul7bihzKYmS0bLqY9EhHvHjLryBarTW9af0bT6zublwG/jYiZ3cxnZmZLz6d5zczMKvLItGYiYla/M5iZ2dN5ZGpmZlaRi6mZmVlFLqZmZmYVuZiamZlV5GJqZmZWke/mHUMmT5ky4nI/1MHMbOl4ZGpmZlaRi2kXSLpyKbfbVdIGFY47VdLQJyuZmVmPuZh2QURsvZSb7gosdTElt2VzMTUz6zNfM+0CSfMjYoKk7YFZwAPkZ+neALy39CQ9lNzkexHwM+DMMr2dpM8A7wJ2JDfzXpHcKPx9EfGopBOAfwCbAS8ADo6I04FDgZdLmgv8MCIO79GXbGZmTVxMu28TYENgALgCeI2k24F3AC8rhXViRPxd0hzgnFIYkfT3iDi2vP4SsC/w7bLftYBtgJcBc4DTgUOAgyLira2CSJpJLs5ERNvgkyZNWrqvuEvGjx/f9wzt1CEj1COnM3ZHHTJCPXJWyehi2n3XRsTdAGXEOBW4GlgAHCfpXOCcYbadXoroRGACuffpoJ9GxJPA7ZLW7CRIRMwGZpfJtg3Eh3aY77VWXe5HmzpkhHrkdMbuqENGqEfOVhknT57c0ba+Ztp9C5teLwbGR8QiYAvgDPJ10guG2fYE4GMR8Qrg88DKw+w3dS2tmZlV5mLaA5ImAKtHxHnAAcDGZdEjwGpNq64G3CtpBeA9Hex66PZmZtYHPs3bG6sBZ0tamTyq/GSZfypwrKT9gd2A/wdcA/wJuIX2hfJmYJGkecAJ7W5A8kMZzMyWjdRotL2UZsuHxsDAQL8zjKiu11RGozrkdMbuqENGqEfOEa6Ztr205tO8ZmZmFbmYmpmZVeRiamZmVpGLqZmZWUUupmZmZhW5mJqZmVXkYmpmZlaRi+kYMnnKFCZPmdLvGGZmyx0XUzMzs4pcTFuQdJ6kiUuw/lRJty7LTCMce34/jmtmZk/xs3lbiIid+53BzMzqY0wWU0kHAwsi4luSDgc2iogdJb0O+AC5Cfdm5J6i5wOXA1sD9wC7RMRjkjYFfgA8WpYP7ntD4HhgRfLI/13AE+S2a9eQm4f/Fnh/RDxa9nNYOdYDwIyIuFfS+sDRwPPLMfaLiF9LehFwMvl7N1wrNzMz66ExWUyBXwIHAt8iF82VStuzbYDLyudB04C9ImI/SUEujieSC+bHI+JSSV9vWv9DwJERcZKkFYFxwJrAS4F9I+IKST8APiLpSODb5AJ9v6Q9gC8D+5Cben8oIn4n6dXAd4AdgSOB70bEjyR9dKQvUtJMYCZARPxr/mjtdl+ly32v1CEj1COnM3ZHHTJCPXJWyThWi+kNwKaSViM33b6RXFS3BfYHPt207h0RMbdpu6mSVgcmRsSlZf6PgZ3K66uA/5a0NnBmKYYAd0XEFWWdE8txLgCmAz8v64wj9zOdQB4J/6TMB1ipfH4NuaAPHverw32RETGbXJQB/tUeaLR2bqhrV4nRqA45nbE76pAR6pFzhK4xbY3JYhoRT0i6k3xK90pyX9AdgPWBXw1ZfWHT68XAKuR2PC1710XEyZKuAd4CXCjpg8AfW6zfKPu5LSK2al4g6TnA3yNiY1pz3zwzs1FkLN/N+0vgoPL5MvLp2bkR0bZQRcTfgYclDZ4Ofs/gMknrAX+MiG8Bc4BXlkXrShosmnuRr7P+Bnj+4HxJK0jaMCL+AdwhafcyP0naqGx7BbDn0OOamVn/jOViehmwFnBVRNwHLCjzOvUB4GhJVwGPNc3fA7hV0lzgZcCPyvxfAXtLuhn4N/J1z8eB3YCvSpoHzCWf3oVcKPct828DdinzPwF8VNJ1wOpL8gWbmdmykRoNnzFc1iRNBc6JiOl9jNEYGBjo4+Hbq+s1ldGoDjmdsTvqkBHqkXOEa6ap3bZjeWRqZmbWFWPyBqRei4g7yXftmpnZcsgjUzMzs4pcTM3MzCpyMTUzM6vIxdTMzKwiF1MzM7OKXEzNzMwqcjE1MzOrqCfFVNL8FvM+JOn9bbabIemoYZb91wjb3SnpFknzJP1M0guWPPUz9jlZ0ukdrHdl+TxV0rs7WP9p60naTNK3qqU1M7Ne6tvINCK+FxE/ar/msIYtpsUOEbERcH2rdSWNW5KDRcRAROzWwXqDz9adCrQtpkPXi4jrI2L/JclmZmb91bcnIEmaBcyPiG9I2hz4PvBPcjeVnZqeYztZ0gXk9mhnRcTBkg4FVikPk78tIkbqnvJLcu/QwRHyYcCbgAMlPVamJwAPADMi4l5JLwa+Bzyf3HZt9/L5nIiYLmkG8A5yj9EXASdHxOcHjxERE4BDgZeXjD8EziL3H1215PpYRFzZYr2bgIMi4q2S/g34AbAe8CgwMyJuLu/dumX+usARpUuNmZn1wWh5nODx5EJxZSmUzTYGNiH3Ff2NpG9HxCGSPjZCv89mbwVuKa9XBW6NiM9KWgG4FNglIu6XtAfwZWAf4CTg0Ig4S9LK5BH8GkP2uwX5EYGPAtdJOjcirm9afgilKAJIejbwhohYIGkacAq5IfnQ9bZv2sfngZsiYldJO5I70Ax+zS8j92Bdrbwv342IJ5oDSpoJzASIiOW6y32v1CEj1COnM3ZHHTJCPXJWydj3YippIrBaGaUBnEwugIMuioiHy7q3Ay8E7upg1xdLWkxu/P2ZMm8xcEZ5/VJyMfy5JIBxwL2SVgOmRMRZABGxoBx76P5/HhEPlmVnAtuQTykPZwXgKEkblxwv6eBr2AZ4V8nxC0nPkzTYdu3ciFgILJT0V2BN4O7mjSNiNjC7TDbq2LFhtKlDRqhHTmfsjjpkhHrkHKFrTFt9L6a0b22zsOn1YjrPvENEDP3OLYiIxU3HvS0itmpeQdJzOtz/0N517XrZfRK4D9iIPNJd0MExWr03g8dZ2vfFzMy6rO9/GhMRDwGPSNqyzNqzw02fKKdql9ZvgOdL2gpA0gqSNoyIfwB3S9q1zF+pnKId6g2S/k3SKsCuwBVDlj9CPgU7aHXg3oh4EngfeSTcar1mvyQ3CR88/ftAyWdmZqNIr0Yzz5bUfArysCHL9wWOlfRP4BLg4Q72ORu4WdKNbW5AaikiHpe0G/Ctcup0PHAEcBu52B0j6QvAE+QbkJ4csovLyTcUvZh8A9LQU7w3A4skzQNOAL4DnCFpd+Bi8s1Wrda7qWkfs4DjJd1Mvja795J+nWZmtuylRqPd2cllT9KEiJhfXh8CrBURn+hzrGGVu3k3i4iP9TvLEmgMDAz0O8OI6npNZTSqQ05n7I46ZIR65Bzhmmm7y5Gj5jrbWyR9mpznT8CM/sYxMzPr3KgophFxGnBav3N0KiJOIJ+SNTMz6/8NSGZmZnXnYmpmZlaRi6mZmVlFLqZmZmYVuZiamZlV5GJqZmZWkYupmZlZRS6mFUiaKunWJVj/hPIIQyQdJ2mDFuvMkHRUN3OamdmyNSoe2jAWRcQH+53BzMy6w8W0unGSjgW2Bu4BdiH3Sv0e8GzgD8A+pTvOv0i6hNwU/HpJHwA+DdwL/JbSXk3S28i9WFcEHiR3kLmf3PFm69LU/Fllmy1btJwzM7MecDGtbhqwV0TsJynIzbwPBj4eEZeWzjOfAw5otbGktYDPA5uSu+VczFOdYy4nF8mGpA8CB0fEgZJOJBfWI4DXA/NaFVJJM4GZABGxXHe575U6ZIR65HTG7qhDRqhHzioZXUyruyMi5pbXNwDrAxMj4tIy74fAT0bY/tXAJRFxP4Ck04CXlGVrA6eVgrsicEeZ/wPgbHIx3Qc4vtWOI2I2uVUdQKOOHRtGmzpkhHrkdMbuqENGqEfOEbrGtOUbkKpb2PR6MTBxKfYxXB+8bwNHRcQrgH8HVgaIiLuA+yTtSC7G5y/FMc3MrEtcTLvvYeAhSduW6fcBl46w/jXA9pKeJ2kFciPyQauTr8PCMxuDHwecCERELK4e28zMlpaL6bKxN/B1STcDGwNfGG7FiLgXmAVcBfwfcGPT4lnATyRdBgw9PzIHmMAwp3jNzKx3UqMx3BlGG80kbQYcHhHbtl05awwMDCzLSJXV9ZrKaFSHnM7YHXXICPXIOcI109RuW9+AVEOSDgE+TL6j18zM+szFtIYi4lDg0H7nMDOzzNdMzczMKnIxNTMzq8jF1MzMrCIXUzMzs4pcTM3MzCpyMTUzM6vIxdTMzKwi/51pD0hqACdGxPvK9Hhy79JrIuKtkt4ObFD+frTV9hsDkyPivJ6FNjOzjnlk2hv/BKZLWqVMv4GnHmBPRMwZrpAWGwM7L8N8ZmZWgUemvXM+8BbgdGAv4BRgWwBJM4DNIuJjknYnNxNfTO5A83ryg/JXkbQN8BXgS8DWEXG/pGcBvyU3ER/dD740M1tOuZj2zqnAZyWdA7yS3OC71UPqPwu8KSLukTQxIh6X9FlKsQWQ9DLyc3mPIBfbea0KqaSZwEyAiFiuu9z3Sh0yQj1yOmN31CEj1CNnlYwupj0SETdLmkoelY507fMK4ARJAZw5zDo/AM4mF9N9GKYNW0TMBmaXyUYdOzaMNnXICPXI6YzdUYeMUI+cI3SNacvXTHtrDvAN8ineliLiQ8BngHWAuZKe12Kdu4D7JO0IvJp8CtnMzPrExbS3fgB8ISJuGW4FSetHxDUR8VlyQ/B1gEeA1YasehxwIhARsXhZBTYzs/Z8mreHIuJu4Mg2q31d0jRyM9qLgHnAn4FDJM0FvhIRp5FHucczzCleMzPrndRoNPqdwZaCpM2AwyOi1U1MrTQGBgaWZaTK6npNZTSqQ05n7I46ZIR65Bzhmmlqt61HpjUk6RDgw+Q7es3MrM9cTGuoPOBhpIc8mJlZD/kGJDMzs4pcTM3MzCpyMTUzM6vIxdTMzKwiF1MzM7OKXEzNzMwqcjE1MzOrqO3fmUp6Abk7yebAQuBO4ADgceCciJje7VCSDgBmR8Sj3d73CMfcGJgcEeeV6Rk0tT2rsN/5ETGhC/m2Bw6KiLdW3ZeZmXXXiCNTSQk4C7gkItaPiA2A/wLW7FYASak0uG52APDsbh2jgwzjgY2BnXt1TDMzW360G5nuADwREd8bnBERcwFKb07K63HkJ/JsD6wEHB0Rx0iaQO67+VxgBeAzEXF22fZ84GJgK2BX4E9lX/sDk4GLJT0QETtI2otcxBNwbkR8amhQSXcCp5XMAO+OiN9Lehu5pdmKwIPAeyLiPkmzynGmkruzbAOsImkb4CtN+10NuBl4SUQ8Iek5ZXpaRDzRtN6awPeA9cqsD0fElU3LE/A1YCegAXwpIk4bOuKUdBRwfUScIOnN5LMCDwA3luXPAn4DbB0R95fp3wJbtmoQbmZmy167a6bTgRs62M++wMMRsTn5dPB+kl4ELADeERGvIhe5b5aiAvBS4EcRsUlE/GlwRxHxLWAA2KEU0snAV4EdyaPHzSXtOkyOf0TEFsBR5CIEcDm50GwCnAoc3LT+psAuEfFu4LPAaRGxcenKMpjnEeAS4C1l1p7AGc2FtPgWcGlEbAS8CrhtyPJ3lvwbAa8nd4dZa5ivA0krA8cCbwO2BV5Q8jxJbr02+Fze1wPzXEio0/kvAAAKWklEQVTNzPqnW8/mfSPwSkm7lenVgWnA3cD/SHot8CQwhadOEf8pIq7uYN+bk08z3w8g6STgtcBPW6x7StPnw8vrtYHTSuFaEbijaf05EfFYBxmOIxfhnwIfAPZrsc6OwPsBSn/Rh4cs3wY4pSy7T9Kl5Wv7xzDHfBlwR0T8DkDSicDMsuwH5BH/EcA+DNOGTdLMwW0igkmTJrX9Qvtp/PjxztgldcjpjN1Rh4xQj5xVMrYrprcBu7VZB/Lp149HxIXNM8tNPM8HNi2nSO8EVi6L/9lhxratb5o0Wrz+NnBYRMwpp1RnNa3TUYaIuELSVEnbAeMi4tYlyDRouK9jEU8/Q7By0+uW/fEi4i5J90naEXg1w3SPiYjZwOzBfdWx/dFoU4eMUI+cztgddcgI9cg5Qgu2ttqd5v0FsJKkf43EJG1eikqzC4EPS1qhrPMSSauSR6h/LYV0B+CFHaWCR4DVyutrgO0kTSrXZvcCLh1muz2aPl9VXq8O3FNe793hMVv5EXnEO1wz7ovIbdGQNK5cW232S2CPsuz55NH1teRrxRtIWknS6sDryvq/Bl4kaf0yvdeQ/R1HPt0bZbRrZmZ9MmIxjYgG8A7gDZL+IOk28shuaJfp44DbgRsl3QocQx71ngRsJul68ujp1x3mmg2cL+niiLgX+DT5ZqV5wI0RcfYw260k6RrgE8Any7xZwE8kXUa+kWc4F5OL2lxJe7RYfhL5RqpTWiyjHHMHSbeQrzNvOGT5WeQbl+aRf0k5OCL+EhF3AVGWnQTcBBARC8inaM+VdDnlBq0mc4AJDF/czcysR1Kj0fJMYu2UU8ibLasbccr14F0i4n3LYv9LStJmwOERsW2HmzQGBob+DjS61PU00GhUh5zO2B11yAj1yDnCad62lxvdHLwDkr5N/pOWUfF3qJIOIZ9Sbnmt1MzMemu5GZlaWx6ZdkEdMkI9cjpjd9QhI9QjZ5WRqZ/Na2ZmVpGLqZmZWUUupmZmZhW5mJqZmVXkYmpmZlaRi6mZmVlFLqZmZmYVuZiOcpImSvpI0/T2ks7pZyYzM3s6F9PRbyLwkbZrmZlZ3/hxgj0gaSpwAaVROflh98cDnwfWID8WcGdgXWC98vmI0ij9UGB9SXOBnwPnAhMknc5TzdvfW5oSmJlZH7iY9s6Lgd3JnWCuA95Nbhj+duC/gLnkhuA7kFvB/UbSd4FDgOkRsTHk07zAJuSuNAPAFcBryIXazMz6wMW0d+6IiFsASiu7iyKiUVq2TSUX03MjYiGwUNJfgTWH2de1EXF32dfcsv0ziqmkmeTiTUQs113ue6UOGaEeOZ2xO+qQEeqRs0pGF9PeWdj0+smm6Sd56vvQvM5ihv/+dLReRMwm94YFaNTxIdOjTR0yQj1yOmN31CEj1CPnCA+6b8s3II1+j5BP+5qZ2SjlYjrKRcSDwBWSbpX09X7nMTOzZ3I/07HD/Uy7oA4ZoR45nbE76pAR6pHT/UzNzMz6yMXUzMysIhdTMzOzilxMzczMKnIxNTMzq8jF1MzMrCIXUzMzs4pcTM3MzCpyMTUzM6vIxdTMzKwiF1MzM7OKXEyXA5LG9TuDmdlY5n6mNSDpi8ADEXFkmf4ycB/wDuBeYGNgg/4lNDMb21xM6+H7wJnAkZKeBewJHAxsAUyPiDtabSRpJjATICKW6y73vVKHjFCPnM7YHXXICPXIWSWjW7DVhKSfkwvomsAHgaOAz0XEDh3uwi3YuqAOGaEeOZ2xO+qQEeqRs0oLNo9M6+M4YAbwAuAHZd4/+5bGzMz+xTcg1cdZwJuBzYEL+5zFzMyauJjWREQ8DlycX8bifucxM7On+DRvTZQbj7YEdgeIiEuAS/oYyczMCo9Ma0DSBsDvgYsi4nf9zmNmZk/nkWkNRMTtwHr9zmFmZq15ZGpmZlaR/8507PA32sxs6bT9O1OPTMcISTeQ/0GM2g9nHFs5nXHsZKxLzhEytuViamZmVpGLqZmZWUUupmPH7H4H6IAzdk8dcjpjd9QhI9Qj51Jn9A1IZmZmFXlkamZmVpGLqZmZWUV+AtJyRNKbgSOBccBxEXHokOUrAT8CNgUeBPaIiDtHYc7XAkcArwT2jIjTR2HG/yD3lV0E3A/sExF/GmUZPwR8FFgMzAdmlqdp9VS7nE3r7Qb8BNg8Iq7vYcRO3ssZwNeBe8qsoyLiuNGUsawjYBb578rnRcS7R1NGSYcDO5TJZwNrRMTEXmYsOdrlXBf4ITCxrHNIRJw30j49Ml1OSBoHHA3sBGwA7FWe6dtsX+ChiHgxcDjw1d6m7Djnn8m9W0/ubbqsw4w3AZtFxCuB04GvjcKMJ0fEKyJi45LvsF5mhI5zImk1YH/gmt4m7DwjcFpEbFw+el1I22aUNA34NPCaiNgQOGC0ZYyITw6+h8C3gTN7mbHTnMBnyB26NgH2BL7Tbr8upsuPLYDfR8QfS7u2U4FdhqyzC/m3LcgF4HWSOvqD5C5qmzMi7oyIm4Ene5xtUCcZL46IR8vk1cDaozDjP5omV6U/T8Hq5N8lwBfJBX9BL8MVnWbsp04y7gccHREPAUTEX0dhxmZ7Aaf0JNnTdZKzATynvF4dGGi3UxfT5ccU4K6m6bvLvJbrRMQi4GHgeT1J1yJD0Spnvy1pxn2B85dpomfqKKOkj0r6A7lQ7d+jbM3a5pS0CbBORJzTy2BNOv1+v0vSzZJOl7ROb6L9SycZXwK8RNIVkq4upzJ7qeP/N5JeCLwI+EUPcg3VSc5ZwHsl3Q2cB3y83U5dTJcfrUaYQ0cinayzrI2GDO10nFHSe4HNyNfTeqmjjBFxdESsD3yKfOqq10bMWfr0Hg4c2LNEz9TJe/m/wNRyWv//eOoMT690knE8MA3YnjzqO05SL69HLsn/7T2B0yNi8TLMM5xOcu4FnBARawM7Az8u/1aH5WK6/LgbaP5teW2eeWriX+tIGk8+ffG3nqRrkaFolbPfOsoo6fXAfwNvj4iFPco2aEnfx1OBXZdpotba5VwNmA5cIulOYEtgjqTNepawg/cyIh5s+h4fS76Jr5c6/f99dkQ8ERF3AL8hF9deWZJ/k3vSn1O80FnOfYEAiIirgJWBSSPt1HfzLj+uA6ZJehH5jsM9gaF38s0B9gauAnYDfhERvR4VdpKz39pmLKcmjwHe3IdrU9BZxmlNzeTfAvSjsfyIOSPiYZp+SEm6BDiox3fzdvJerhUR95bJtwO/6mE+6Oz/zU8pIypJk8inff84yjIi6aXAc8k/h/qhk5x/Bl5Hfi9fTi6m94+0U49MlxPlGujHgAvJ/9EjIm6T9AVJby+rfR94nqTfA/8BHDIac0ravFyr2B04RtJtoy0j+bTuBOAnkuZKmjMKM35M0m2S5pK/33v3MuMS5OyrDjPuX97LeeRrzzNGYcYLgQcl3Q5cDPxnRDw4yjJCLvin9uEX+SXJeSCwX/l+nwLMaJfXjxM0MzOryCNTMzOzilxMzczMKnIxNTMzq8jF1MzMrCIXUzMzs4pcTM3MzCpyMTUzM6vo/wOaYUyRuPEZkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a pd.Series of features importances\n",
    "importances = pd.Series(data=rf.feature_importances_,\n",
    "                        index= X_train.columns)\n",
    "\n",
    "# Sort importances\n",
    "importances_sorted = importances.sort_values()\n",
    "\n",
    "# Draw a horizontal barplot of importances_sorted\n",
    "importances_sorted.plot(kind='barh', color='red')\n",
    "plt.title('Features Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## AdaBoost\n",
    "\n",
    "**Boosting** (originally called hypothesis boosting) refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor.\n",
    "\n",
    "**Weak Learning**: Model doing slightly better than random guessing. Ex: Decision tree with max depth = 1\n",
    "\n",
    "Two main methods of boosting:\n",
    "1. *AdaBoost*\n",
    "2. *Gradient Boosting*\n",
    "\n",
    "**AdaBoost** stands for adaptive boosting.\n",
    "\n",
    "Each predictor pays more attendtion to the instances wrongly predicted by its predecssor. This is achieved by changing weights of training instances. Depending on the predictor's training error, a coefficienct $\\alpha$ is assigned to the predictor. This coefficient will determine the weights of the next training instances, in which wrongly modeled instances will have higher weight so that the next predictor will pay more attention to it. Thus, reducing its error. \n",
    "\n",
    "An important parameter  0 < $\\eta$ <= 1 called the **Learning Rate** and used to shrink the coefficient $\\alpha$. It is important to note that there is a tradeoff between $\\alpha$ and the number of estimators.\n",
    "\n",
    "In the next example you will work on the liver dataset, but this time **ROC AUC** is chosen as the metric (see `sklearn` notebook), and **`.predict_proba()`** is used to find probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age_std</th>\n",
       "      <th>Total_Bilirubin_std</th>\n",
       "      <th>Direct_Bilirubin_std</th>\n",
       "      <th>Alkaline_Phosphotase_std</th>\n",
       "      <th>Alamine_Aminotransferase_std</th>\n",
       "      <th>Aspartate_Aminotransferase_std</th>\n",
       "      <th>Total_Protiens_std</th>\n",
       "      <th>Albumin_std</th>\n",
       "      <th>Albumin_and_Globulin_Ratio_std</th>\n",
       "      <th>Is_male_std</th>\n",
       "      <th>Liver_disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.247403</td>\n",
       "      <td>-0.420320</td>\n",
       "      <td>-0.495414</td>\n",
       "      <td>-0.428870</td>\n",
       "      <td>-0.355832</td>\n",
       "      <td>-0.319111</td>\n",
       "      <td>0.293722</td>\n",
       "      <td>0.203446</td>\n",
       "      <td>-0.147390</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.062306</td>\n",
       "      <td>1.218936</td>\n",
       "      <td>1.423518</td>\n",
       "      <td>1.675083</td>\n",
       "      <td>-0.093573</td>\n",
       "      <td>-0.035962</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>0.077462</td>\n",
       "      <td>-0.648461</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.062306</td>\n",
       "      <td>0.640375</td>\n",
       "      <td>0.926017</td>\n",
       "      <td>0.816243</td>\n",
       "      <td>-0.115428</td>\n",
       "      <td>-0.146459</td>\n",
       "      <td>0.478274</td>\n",
       "      <td>0.203446</td>\n",
       "      <td>-0.178707</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age_std  Total_Bilirubin_std  Direct_Bilirubin_std  \\\n",
       "0  1.247403            -0.420320             -0.495414   \n",
       "1  1.062306             1.218936              1.423518   \n",
       "2  1.062306             0.640375              0.926017   \n",
       "\n",
       "   Alkaline_Phosphotase_std  Alamine_Aminotransferase_std  \\\n",
       "0                 -0.428870                     -0.355832   \n",
       "1                  1.675083                     -0.093573   \n",
       "2                  0.816243                     -0.115428   \n",
       "\n",
       "   Aspartate_Aminotransferase_std  Total_Protiens_std  Albumin_std  \\\n",
       "0                       -0.319111            0.293722     0.203446   \n",
       "1                       -0.035962            0.939655     0.077462   \n",
       "2                       -0.146459            0.478274     0.203446   \n",
       "\n",
       "   Albumin_and_Globulin_Ratio_std  Is_male_std  Liver_disease  \n",
       "0                       -0.147390            0              1  \n",
       "1                       -0.648461            1              1  \n",
       "2                       -0.178707            1              1  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets\\indian_liver_patient\\indian_liver_patient_preprocessed.csv'\n",
    "                 ,index_col=0)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Liver_disease',axis=1)\n",
    "y = df['Liver_disease']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,\n",
    "                                                 stratify=y,\n",
    "                                                 random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "\n",
    "# Instantiate ada\n",
    "ada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)\n",
    "\n",
    "# Fit ada to the training set\n",
    "ada.fit(X_train,y_train)\n",
    "\n",
    "# Compute the probabilities of obtaining the positive class\n",
    "y_pred_proba = ada.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.71\n"
     ]
    }
   ],
   "source": [
    "# Import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Evaluate test-set roc_auc_score\n",
    "ada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print roc_auc_score\n",
    "print('ROC AUC score: {:.2f}'.format(ada_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! This untuned AdaBoost classifier achieved a ROC AUC score of 0.71!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting (GB)\n",
    "\n",
    "Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the _residual errors_ as labels made by the previous predictor.\n",
    "\n",
    "The learning rate $\\eta$ also applies here to the residual errors $r$. If you set it to a low value, such as 0.1, you will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. This is a regularization technique called **shrinkage**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/bike.csv')\n",
    "X = df.drop('cnt',axis=1)\n",
    "y = df['cnt']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,\n",
    "                                                random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate gb\n",
    "gb = GradientBoostingRegressor(max_depth=4, \n",
    "            n_estimators=200,\n",
    "            random_state=2)\n",
    "\n",
    "# Fit gb to the training set\n",
    "gb.fit(X_train,y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of gb: 49.796\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute MSE\n",
    "mse_test = MSE(y_test,y_pred)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_test = (mse_test)**(0.5)\n",
    "\n",
    "# Print RMSE\n",
    "print('Test set RMSE of gb: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boosting\n",
    "\n",
    "Most of the time Gradient Boosting involves exchausting search to find the best split point and feature. Also, the same split point might be used again and possibly the same features.\n",
    "\n",
    "You can use **Stochastic Gradient Boosting** in which each CART is trained on a random subset of rows of the training data (without replacement). Also, features are sampled (also without replacement) when chosing split points. This adds further diversity, it also adds more variance.\n",
    "\n",
    "The code is described below:\n",
    "```python\n",
    "GradientBoostingRegressor(max_depth=4, \n",
    "            subsample=0.9,  # keep at least 90% when subsampling\n",
    "            max_features=0.75, # max percentage of features to keep when ranomizing\n",
    "            n_estimators=200,                                \n",
    "            random_state=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate sgbr\n",
    "sgbr = GradientBoostingRegressor(max_depth=4, \n",
    "            subsample=0.9,\n",
    "            max_features=0.75,\n",
    "            n_estimators=200,                                \n",
    "            random_state=2)\n",
    "\n",
    "# Fit sgbr to the training set\n",
    "sgbr.fit(X_train,y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = sgbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of sgbr: 47.944\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute test set MSE\n",
    "mse_test = MSE(y_pred,y_test)\n",
    "\n",
    "# Compute test set RMSE\n",
    "rmse_test = mse_test**(0.5)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stochastic gradient boosting regressor achieves a lower test set RMSE than the gradient boosting regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## CART's Hyperparameters Tuning \n",
    "\n",
    "**Grid Search** Explained in details in `sklearn` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'presort', 'random_state', 'splitter'])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTreeClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets\\indian_liver_patient\\indian_liver_patient_preprocessed.csv'\n",
    "                 ,index_col=0)\n",
    "\n",
    "X = df.drop('Liver_disease',axis=1)\n",
    "y = df['Liver_disease']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,\n",
    "                                                 stratify=y,\n",
    "                                                 random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set ROC AUC score: 0.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import roc_auc_score from sklearn.metrics \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define params_dt\n",
    "params_dt = {'max_depth':(2,3,4),\n",
    "             'min_samples_leaf':(0.12,0.14,0.16,0.18)}\n",
    "\n",
    "\n",
    "# Instantiate grid_dt\n",
    "grid_dt = GridSearchCV(estimator=dt,\n",
    "                       param_grid=params_dt,\n",
    "                       scoring='roc_auc',\n",
    "                       cv=5,\n",
    "                       n_jobs=-1)\n",
    "\n",
    "# Fit the CV\n",
    "grid_dt.fit(X_train,y_train)\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# Predict the test set probabilities of the positive class\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute test_roc_auc\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print test_roc_auc\n",
    "print('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF's Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bootstrap', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestRegressor().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/bike.csv')\n",
    "X = df.drop('cnt',axis=1)\n",
    "y = df['cnt']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,\n",
    "                                                random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE of best model: 54.011\n"
     ]
    }
   ],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initiate the RF model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Define the dictionary 'params_rf'\n",
    "params_rf = {'n_estimators':(100,350,500),\n",
    "             'max_features':('log2','auto','sqrt'),\n",
    "             'min_samples_leaf':(2,10,30)}\n",
    "\n",
    "# Instantiate grid_rf\n",
    "grid_rf = GridSearchCV(estimator=rf,\n",
    "                       param_grid=params_rf,\n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       cv=3,\n",
    "                       n_jobs=-1)\n",
    "\n",
    "# Fit the CV\n",
    "grid_rf.fit(X_train,y_train)\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute rmse_test\n",
    "rmse_test = MSE(y_pred,y_test)**(0.5)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test RMSE of best model: {:.3f}'.format(rmse_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=2, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 'auto', 'min_samples_leaf': 2, 'n_estimators': 500}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
